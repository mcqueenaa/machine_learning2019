{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "assignment_1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a710LN31b_u-",
        "colab_type": "text"
      },
      "source": [
        "# Assignment 1\n",
        "\n",
        "Using text http://www.gutenberg.org/files/2600/2600-0.txt\n",
        "1. Make text lowercase and remove all punctuation except spaces and dots.\n",
        "2. Tokenize text by BPE with vocab_size = 100\n",
        "3. Train 3-gram language model with laplace smoothing $\\delta=1$\n",
        "4. Using beam search with k=10 generate sequences of length=10 conditioned on provided inputs. Treat dots as terminal tokens.\n",
        "5. Calculate perplexity of the language model for the first sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hlFS7MoU_Oe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import collections\n",
        "from collections import Counter\n",
        "from operator import itemgetter"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRU7w6Wgca2h",
        "colab_type": "code",
        "outputId": "9b518afe-9b2a-4f72-b74c-4068c08ab5ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEHvXrswb_vB",
        "colab_type": "code",
        "outputId": "9fe1b194-5e7f-44e2-9844-9648639ef500",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "text = open('/content/drive/My Drive/Colab Notebooks/peace.txt', 'r', encoding = 'utf-8').read()[2:]\n",
        "len(text)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3227579"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "diEiAxExb_vH",
        "colab_type": "code",
        "outputId": "a8adc61f-576c-4a47-beb0-a3845114a052",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import string\n",
        "import re\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # TODO\n",
        "    # make lowercase\n",
        "    # replace all punctuation except dots with spaces\n",
        "    # collapse multiple spaces into one '   ' -> ' '\n",
        "    \n",
        "    text = text.lower()\n",
        "    punct = r'[\\(\\)\\*\\,\\-\\/\\:\\;\\=!\\?#$%@\\[\\]\\—\\‘\\’\\“\\”]'\n",
        "    mult_dot = r'\\.{2,}'\n",
        "    respace = r' {2,}'\n",
        "    respace_1 = r'\\n '\n",
        "    respace_2 = r' \\.'\n",
        "    re_newstr = r'\\n{2,}'\n",
        "    text = re.sub(punct, ' ', text)\n",
        "    text = re.sub(mult_dot, '.', text)\n",
        "    text = re.sub(respace, ' ', text)\n",
        "    text = re.sub(respace_1, '\\n', text)\n",
        "    text = re.sub(respace_2, '.', text)\n",
        "    text = re.sub(re_newstr, '\\n', text)\n",
        "    return text\n",
        "\n",
        "text = preprocess_text(text)\n",
        "#assert len(text) == 3141169\n",
        "len(text)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3143402"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WD1q1UpZcV7v",
        "colab": {}
      },
      "source": [
        "text = re.sub('\\n', '', text)\n",
        "text = text.split('.')\n",
        "text = [x.strip() for x in text]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FUo9I8P0eVG",
        "colab_type": "code",
        "outputId": "bf197c20-dff2-40b6-ba94-8eb1d67e8a04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "source": [
        "text[:10]"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['the project gutenberg ebook of war and peace by leo tolstoythis ebook is for the use of anyone anywhere at no cost and with almostno restrictions whatsoever',\n",
              " 'you may copy it give it away or re useit under the terms of the project gutenberg license included with thisebook or online at www',\n",
              " 'gutenberg',\n",
              " 'orgtitle war and peaceauthor leo tolstoytranslators louise and aylmer maudeposting date january 10 2009 ebook 2600 last updated january 21 2019language englishcharacter set encoding utf 8start of this project gutenberg ebook war and peace an anonymous volunteer and david widgerwar and peaceby leo tolstoy tolstoicontentsbook one 1805chapter ichapter iichapter iiichapter ivchapter vchapter vichapter viichapter viiichapter ixchapter xchapter xichapter xiichapter xiiichapter xivchapter xvchapter xvichapter xviichapter xviiichapter xixchapter xxchapter xxichapter xxiichapter xxiiichapter xxivchapter xxvchapter xxvichapter xxviichapter xxviiibook two 1805chapter ichapter iichapter iiichapter ivchapter vchapter vichapter viichapter viiichapter ixchapter xchapter xichapter xiichapter xiiichapter xivchapter xvchapter xvichapter xviichapter xviiichapter xixchapter xxchapter xxibook three 1805chapter ichapter iichapter iiichapter ivchapter vchapter vichapter viichapter viiichapter ixchapter xchapter xichapter xiichapter xiiichapter xivchapter xvchapter xvichapter xviichapter xviiichapter xixbook four 1806chapter ichapter iichapter iiichapter ivchapter vchapter vichapter viichapter viiichapter ixchapter xchapter xichapter xiichapter xiiichapter xivchapter xvchapter xvibook five 1806 07chapter ichapter iichapter iiichapter ivchapter vchapter vichapter viichapter viiichapter ixchapter xchapter xichapter xiichapter xiiichapter xivchapter xvchapter xvichapter xviichapter xviiichapter xixchapter xxchapter xxichapter xxiibook six 1808 10chapter ichapter iichapter iiichapter ivchapter vchapter vichapter viichapter viiichapter ixchapter xchapter xichapter xiichapter xiiichapter xivchapter xvchapter xvichapter xviichapter xviiichapter xixchapter xxchapter xxichapter xxiichapter xxiiichapter xxivchapter xxvchapter xxvibook seven 1810 11chapter ichapter iichapter iiichapter ivchapter vchapter vichapter viichapter viiichapter ixchapter xchapter xichapter xiichapter xiiibook eight 1811 12chapter ichapter iichapter iiichapter ivchapter vchapter vichapter viichapter viiichapter ixchapter xchapter xichapter xiichapter xiiichapter xivchapter xvchapter xvichapter xviichapter xviiichapter xixchapter xxchapter xxichapter xxiibook nine 1812chapter ichapter iichapter iiichapter ivchapter vchapter vichapter viichapter viiichapter ixchapter xchapter xichapter xiichapter xiiichapter xivchapter xvchapter xvichapter xviichapter xviiichapter xixchapter xxchapter xxichapter xxiichapter xxiiibook ten 1812chapter ichapter iichapter iiichapter ivchapter vchapter vichapter viichapter viiichapter ixchapter xchapter xichapter xiichapter xiiichapter xivchapter xvchapter xvichapter xviichapter xviiichapter xixchapter xxchapter xxichapter xxiichapter xxiiichapter xxivchapter xxvchapter xxvichapter xxviichapter xxviiichapter xxixchapter xxxchapter xxxichapter xxxiichapter xxxiiichapter xxxivchapter xxxvchapter xxxvichapter xxxviichapter xxxviiichapter xxxixbook eleven 1812chapter ichapter iichapter iiichapter ivchapter vchapter vichapter viichapter viiichapter ixchapter xchapter xichapter xiichapter xiiichapter xivchapter xvchapter xvichapter xviichapter xviiichapter xixchapter xxchapter xxichapter xxiichapter xxiiichapter xxivchapter xxvchapter xxvichapter xxviichapter xxviiichapter xxixchapter xxxchapter xxxichapter xxxiichapter xxxiiichapter xxxivbook twelve 1812chapter ichapter iichapter iiichapter ivchapter vchapter vichapter viichapter viiichapter ixchapter xchapter xichapter xiichapter xiiichapter xivchapter xvchapter xvibook thirteen 1812chapter ichapter iichapter iiichapter ivchapter vchapter vichapter viichapter viiichapter ixchapter xchapter xichapter xiichapter xiiichapter xivchapter xvchapter xvichapter xviichapter xviiichapter xixbook fourteen 1812chapter ichapter iichapter iiichapter ivchapter vchapter vichapter viichapter viiichapter ixchapter xchapter xichapter xiichapter xiiichapter xivchapter xvchapter xvichapter xviichapter xviiichapter xixbook fifteen 1812 13chapter ichapter iichapter iiichapter ivchapter vchapter vichapter viichapter viiichapter ixchapter xchapter xichapter xiichapter xiiichapter xivchapter xvchapter xvichapter xviichapter xviiichapter xixchapter xxfirst epilogue 1813 20chapter ichapter iichapter iiichapter ivchapter vchapter vichapter viichapter viiichapter ixchapter xchapter xichapter xiichapter xiiichapter xivchapter xvchapter xvisecond epiloguechapter ichapter iichapter iiichapter ivchapter vchapter vichapter viichapter viiichapter ixchapter xchapter xichapter xiibook one 1805chapter iwell prince so genoa and lucca are now just family estates of thebuonapartes',\n",
              " 'but i warn you if you don t tell me that this means war if you still try to defend the infamies and horrors perpetrated by thatantichrist i really believe he is antichrist i will have nothingmore to do with you and you are no longer my friend no longer myfaithful slave as you call yourself but how do you do i see ihave frightened you sit down and tell me all the news',\n",
              " 'it was in july 1805 and the speaker was the well known anna pávlovnaschérer maid of honor and favorite of the empress márya fëdorovna',\n",
              " 'with these words she greeted prince vasíli kurágin a man of highrank and importance who was the first to arrive at her reception',\n",
              " 'annapávlovna had had a cough for some days',\n",
              " 'she was as she said sufferingfrom la grippe grippe being then a new word in st',\n",
              " 'petersburg usedonly by the elite']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "Di49EBLib_vN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#from collections import Counter\n",
        "import nltk\n",
        "from sklearn.base import TransformerMixin\n",
        "\n",
        "\n",
        "class BPE(TransformerMixin):\n",
        "    def __init__(self, vocab_size=100):\n",
        "        super(BPE, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        # index to token\n",
        "        self.itos = []\n",
        "        # token to index\n",
        "        self.stoi = {}\n",
        "        \n",
        "    def fit(self, text):\n",
        "        \"\"\"\n",
        "        fit itos and stoi\n",
        "        text: list of strings \n",
        "        \"\"\"\n",
        "        \n",
        "        # TODO\n",
        "        # tokenize text by symbols and fill in self.itos and self.stoi\n",
        "        items = list([i for sent in text for i in sent])\n",
        "        self.itos = list(set(items))\n",
        "        self.stoi = {letter: idx for idx, letter in enumerate(self.itos)}\n",
        "        new_text = []\n",
        "        for sent in text:\n",
        "          new_text.append([self.stoi[letter] for letter in sent])\n",
        "        text = new_text\n",
        "        \n",
        "        while len(self.itos) < self.vocab_size:\n",
        "            # TODO\n",
        "            # count bigram freqencies in the text\n",
        "            bigrams = collections.Counter()\n",
        "            for sent in text:\n",
        "              for i in range(len(sent)):\n",
        "                if i + 1 < len(sent):\n",
        "                  bigrams[(sent[i], sent[i+1])] += 1\n",
        "            \n",
        "            mostcom_bi = bigrams.most_common(1)[0][0]\n",
        "            f_of_bi = str(self.itos[int(mostcom_bi[0])])\n",
        "            s_of_bi = str(self.itos[int(mostcom_bi[1])])\n",
        "            new_token = f_of_bi + s_of_bi\n",
        "            new_id = len(self.itos)\n",
        "            \n",
        "            self.itos.append(new_token)\n",
        "            self.stoi[new_token] = new_id\n",
        "            \n",
        "            # find occurences of the new_token in the text and replace them with new_id\n",
        "            new_text = []\n",
        "            for sent in text:\n",
        "              new_sent = []\n",
        "              jump = -1\n",
        "              for i in range(len(sent)):\n",
        "                if i + 1 < len(sent):\n",
        "                  if i != jump:\n",
        "                    if new_token == self.itos[sent[i]] + self.itos[sent[i+1]]:\n",
        "                      new_sent.append(new_id)\n",
        "                      jump = i + 1\n",
        "                    else:\n",
        "                      new_sent.append(sent[i])\n",
        "                else: \n",
        "                  if i != jump:\n",
        "                    new_sent.append(sent[i])                    \n",
        "              new_text.append(new_sent)\n",
        "              \n",
        "            text = new_text\n",
        "            \n",
        "        return self\n",
        "    \n",
        "    def transform(self, text):\n",
        "        \"\"\"\n",
        "        convert text to a sequence of token ids\n",
        "        text: list of strings\n",
        "        \"\"\"\n",
        "        # TODO tokenize text by symbols using self.stoi\n",
        "        new_text0 = []\n",
        "        for sent in text:\n",
        "          new_text0.append([self.stoi[letter] for letter in sent])\n",
        "        text = new_text0\n",
        "        \n",
        "        # find occurences of the token in the text and replace them with token_id\n",
        "        for token_id, token in enumerate(self.itos):\n",
        "          new_text = []\n",
        "          for sent in text:\n",
        "            new_sent = []\n",
        "            jump = -1\n",
        "            for i in range(len(sent)):\n",
        "              if i + 1 < len(sent):\n",
        "                if i != jump:\n",
        "                  if token == self.itos[sent[i]] + self.itos[sent[i+1]]:\n",
        "                    new_sent.append(token_id)\n",
        "                    jump = i + 1\n",
        "                  else:\n",
        "                    new_sent.append(sent[i])\n",
        "              else: \n",
        "                if i != jump:\n",
        "                  new_sent.append(sent[i])                    \n",
        "            new_text.append(new_sent)\n",
        "              \n",
        "            text = new_text\n",
        "       \n",
        "        return text\n",
        "    \n",
        "    def decode_token(self, tok):\n",
        "        \"\"\"\n",
        "        tok: int or tuple\n",
        "        \"\"\"\n",
        "        if isinstance(tok, int):\n",
        "          result = self.itos[tok]\n",
        "        else:\n",
        "          result = []\n",
        "          for i in token:\n",
        "            result.append(self.itos[i])\n",
        "        return result\n",
        "            \n",
        "    def decode(self, text):\n",
        "        \"\"\"\n",
        "        convert token ids into text\n",
        "        \"\"\"\n",
        "        return ''.join(map(self.decode_token, text))\n",
        "        \n",
        "        \n",
        "vocab_size = 100\n",
        "bpe = BPE(vocab_size)\n",
        "tokenized_text = bpe.fit_transform(text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRf9otxZb_vQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "assert bpe.decode(tokenized_text[0]) == text[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNts8HWUb_vS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "        \n",
        "    \n",
        "start_token = vocab_size\n",
        "end_token = vocab_size \n",
        "        \n",
        "    \n",
        "class LM:\n",
        "    def __init__(self, vocab_size, delta=1):\n",
        "        self.delta = delta\n",
        "        self.vocab_size = vocab_size + 2\n",
        "        self.proba = Counter() # TODO create array for storing 3-gram counters\n",
        "        self.proba_ab = Counter()\n",
        "        \n",
        "    def infer(self, a, b, tau=1):\n",
        "        \"\"\"\n",
        "        return vector of probabilities of size self.vocab for 3-grams which start with (a,b) tokens\n",
        "        a: first token id\n",
        "        b: second token id\n",
        "        tau: temperature\n",
        "        \"\"\"\n",
        "        result = []\n",
        "        for item in range(self.vocab_size):\n",
        "            result.append(self.get_proba(a, b, item, tau))\n",
        "        return result\n",
        "        \n",
        "    def get_proba(self, a, b, c, tau=1):\n",
        "        \"\"\"\n",
        "        get probability of 3-gram (a,b,c)\n",
        "        a: first token id\n",
        "        b: second token id\n",
        "        c: third token id\n",
        "        tau: temperature\n",
        "        \"\"\"\n",
        "        c_abc = self.proba[a, b, c]     \n",
        "        c_ab = self.proba_ab[a, b]\n",
        "        result = ((c_abc + self.delta) ** (1/tau))/((c_ab + self.delta) ** (1/tau))\n",
        "        return result\n",
        "    \n",
        "    def fit(self, text):\n",
        "        \"\"\"\n",
        "        train language model on text\n",
        "        text: list of lists\n",
        "        \"\"\"\n",
        "        for sent in text:\n",
        "            for i in range(len(sent)):\n",
        "              if i <= len(sent) - 3:\n",
        "                self.proba[sent[i], sent[i+1], sent[i+2]] += 1                     \n",
        "        # TODO count 3-grams in the text\n",
        "        \n",
        "        for sent in text:\n",
        "            for i in range(len(sent)):\n",
        "              if i <= len(sent) - 2:\n",
        "                self.proba_ab[sent[i], sent[i+1]] += 1 \n",
        "        \n",
        "        return self\n",
        "    \n",
        "lm = LM(vocab_size, 1).fit(tokenized_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIUo-_M-b_vV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def beam_search(input_seq, lm, max_len=10, k=5, tau=1):\n",
        "    \"\"\"\n",
        "    generate sequence from language model *lm* conditioned on input_seq\n",
        "    input_seq: sequence of token ids for conditioning\n",
        "    lm: language model\n",
        "    max_len: max generated sequence length\n",
        "    k: size of beam\n",
        "    tau: temperature\n",
        "    \"\"\"\n",
        "    \n",
        "    beam = [(input_seq, 1)]\n",
        "    for i in range(max_len):\n",
        "      candidates = []\n",
        "      candidates_proba = []\n",
        "      \n",
        "      for tup in beam:\n",
        "        seq = tup[0]\n",
        "        if seq[-2] != end_token:\n",
        "          best_k = []\n",
        "          prob = lm.infer(seq[-2], seq[-1], tau)\n",
        "          sort_prob = sorted([(p, idx) for idx, p in enumerate(prob)], reverse = True)\n",
        "          for idx in sort_prob[:k]:\n",
        "            if idx[1] < end_token:\n",
        "              best_k.append(bpe.itos[idx[1]]) \n",
        "          for tok in best_k:\n",
        "            candidates.append(seq + [bpe.stoi[tok]])\n",
        "            candidates_proba.append(sort_prob[bpe.stoi[tok]][0])      \n",
        "      beam = [(candidates[u], candidates_proba[u]) for u in range(k)]\n",
        "  \n",
        "    return beam\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8STcP73b_vY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "0a87d69b-b141-411b-8d1e-5b3a138179c3"
      },
      "source": [
        "input1 = 'horse '\n",
        "input1 = bpe.transform([input1])[0]\n",
        "result = beam_search(input1, lm, max_len=10, k=10, tau=0.1)\n",
        "# TODO print decoded generated strings and their probabilities\n",
        "for res in result:\n",
        "  print(bpe.decode(tuple([i for i in res[0]])), res[1])"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "horse was sold not b 5.102756064303754e-15\n",
            "horse was sold not s 6.599707150644614e-20\n",
            "horse was sold not to  9.251801682952108e-38\n",
            "horse was sold not l 1.2754394268978178e-26\n",
            "horse was sold not k 1.0826425087923193e-15\n",
            "horse was sold not re 9.251801682952108e-38\n",
            "horse was sold not u 4.842240858492817e-17\n",
            "horse was sold not f 1.0453862326160329e-11\n",
            "horse was sold not m 9.473844923342959e-35\n",
            "horse was sold not c 1.302161333895674e-15\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4T_ZFkoZb_vb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "0e3d2b95-b984-4fe4-dcda-2c783923ba23"
      },
      "source": [
        "input1 = 'her'\n",
        "input1 = bpe.transform([input1])[0]\n",
        "result = beam_search(input1, lm, max_len=10, k=10, tau=0.1)\n",
        "# TODO print decoded generated strings and their probabilities\n",
        "for res in result:\n",
        "  print(bpe.decode(tuple([i for i in res[0]])), res[1])"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "here was sold not  7.007302040006144e-39\n",
            "here was sold not 2.5019342280897436e-26\n",
            "here was sold noth 7.007302040006144e-39\n",
            "here was sold nov 7.007302040006144e-39\n",
            "here was sold nob 1.4812895690103165e-20\n",
            "here was sold noi 7.007302040006144e-39\n",
            "here was sold nos 7.175477288966291e-36\n",
            "here was sold noc 8.341662193345087e-22\n",
            "here was sold nod 4.137741781603228e-34\n",
            "here was sold nol 7.007302040006144e-39\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rc5WWrsb_vf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "67eb9ab3-319c-4014-d129-69151d674b07"
      },
      "source": [
        "input1 = 'what'\n",
        "input1 = bpe.transform([input1])[0]\n",
        "result = beam_search(input1, lm, max_len=10, k=10, tau=1)\n",
        "# TODO print decoded generated strings and their probabilities\n",
        "for res in result:\n",
        "  print(bpe.decode(tuple([i for i in res[0]])), res[1])"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "whatess mary to the cou 0.0003184713375796178\n",
            "whatess mary to the con 0.0003184713375796178\n",
            "whatess mary to the ca 0.0035031847133757963\n",
            "whatess mary to the com 0.0003184713375796178\n",
            "whatess mary to the co 0.0070063694267515925\n",
            "whatess mary to the cr 0.0003184713375796178\n",
            "whatess mary to the chi 0.0003184713375796178\n",
            "whatess mary to the cl 0.0003184713375796178\n",
            "whatess mary to the car 0.0003184713375796178\n",
            "whatess mary to the cor 0.0003184713375796178\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwEyxTYRb_vi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "02328112-c24f-4aa2-9817-1d910e2e5cad"
      },
      "source": [
        "input1 = 'gun '\n",
        "input1 = bpe.transform([input1])[0]\n",
        "result = beam_search(input1, lm, max_len=10, k=10, tau=0.1)\n",
        "# TODO print decoded generated strings and their probabilities\n",
        "for res in result:\n",
        "  print(bpe.decode(tuple([i for i in res[0]])), res[1])"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gun and so musside  1.5074038602896436e-36\n",
            "gun and so mussiden 1.5074038602896436e-36\n",
            "gun and so musside 8.901069054624316e-32\n",
            "gun and so mussider 1.5074038602896436e-36\n",
            "gun and so mussidl 8.901069054624316e-32\n",
            "gun and so mussided  1.5074038602896436e-36\n",
            "gun and so mussidg 8.692450248656558e-25\n",
            "gun and so mussidd 4.360235835175713e-25\n",
            "gun and so mussidn 1.6971858658743216e-21\n",
            "gun and so mussiding  1.5074038602896436e-36\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZX2j2I3b_vm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b8835931-824f-4661-c208-5f9c47214407"
      },
      "source": [
        "def perplexity(snt, lm):\n",
        "    \"\"\"\n",
        "    snt: sequence of token ids\n",
        "    lm: language model\n",
        "    \"\"\"\n",
        "    p = 1\n",
        "    for i in range(len(snt)):\n",
        "      if i <= len(snt) - 3:\n",
        "        p *= lm.get_proba(snt[i], snt[i + 1], snt[i + 2])\n",
        "    m = -1 / len(snt)\n",
        "    result = p ** m #TODO perplexity for the sentence\n",
        "    return result\n",
        "\n",
        "perplexity(tokenized_text[0], lm)"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12.472877566841332"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 127
        }
      ]
    }
  ]
}